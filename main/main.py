# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
# flake8: noqa: E402
from __future__ import annotations  # å¯¼å…¥æœªæ¥ç‰ˆæœ¬çš„æ³¨è§£åŠŸèƒ½ï¼Œå…è®¸åœ¨ç±»å‹æç¤ºä¸­ä½¿ç”¨å°šæœªå®šä¹‰çš„ç±»

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import asyncio  # å¯¼å…¥å¼‚æ­¥IOæ¨¡å—ï¼Œç”¨äºå¤„ç†å¼‚æ­¥ç¼–ç¨‹
import json  # å¯¼å…¥JSONå¤„ç†æ¨¡å—
import logging  # å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—
import os  # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—
import random  # å¯¼å…¥éšæœºæ•°ç”Ÿæˆæ¨¡å—
import sys  # å¯¼å…¥ç³»ç»Ÿç‰¹å®šå‚æ•°å’Œå‡½æ•°æ¨¡å—
import time  # å¯¼å…¥æ—¶é—´æ¨¡å—ï¼Œç”¨äºç”Ÿæˆéšæœºç§å­
import warnings  # å¯¼å…¥è­¦å‘Šæ¨¡å—ï¼Œç”¨äºæ§åˆ¶è­¦å‘Šä¿¡æ¯
from datetime import datetime, timedelta  # ä»datetimeæ¨¡å—å¯¼å…¥æ—¥æœŸæ—¶é—´å’Œæ—¶é—´å·®ç±»
from typing import Any  # ä»typingæ¨¡å—å¯¼å…¥Anyç±»å‹ï¼Œç”¨äºç±»å‹æç¤º
import sqlite3  # å¯¼å…¥sqlite3æ¨¡å—ï¼Œç”¨äºè®¿é—®æ•°æ®åº“
import pdb
from colorama import Back, Fore, Style # ä»coloramaå¯¼å…¥Backï¼Œç”¨äºæ§åˆ¶å°æ–‡æœ¬èƒŒæ™¯è‰²
from yaml import safe_load  # ä»yamlå¯¼å…¥safe_loadå‡½æ•°ï¼Œç”¨äºå®‰å…¨åŠ è½½YAMLæ–‡ä»¶
from tqdm import tqdm  # å¯¼å…¥tqdmç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
from utils import *


# ç¦ç”¨å¼‚æ­¥è­¦å‘Š
warnings.filterwarnings("ignore", category=RuntimeWarning, module="asyncio")
logging.getLogger("asyncio").setLevel(logging.ERROR)

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ä¸­ï¼Œä»¥ä¾¿å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

# é¢å¤–ç¡®ä¿å½“å‰ç›®å½•ä¹Ÿåœ¨è·¯å¾„ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# å¯¼å…¥é¡¹ç›®ä¸­çš„å„ä¸ªæ¨¡å—
from asce.clock.clock import Clock  # å¯¼å…¥æ—¶é’Ÿæ¨¡å—
from asce.social_agent.agents_generator import (gen_control_agents_with_data,
                                                 generate_reddit_agents)  # å¯¼å…¥ä»£ç†ç”Ÿæˆå™¨
from asce.social_platform.channel import Channel  # å¯¼å…¥é€šä¿¡é€šé“æ¨¡å—
from asce.social_platform.platform import Platform  # å¯¼å…¥å¹³å°æ¨¡å—
from asce.social_platform.typing import ActionType  # å¯¼å…¥åŠ¨ä½œç±»å‹æšä¸¾

# å¯¼å…¥è®¤çŸ¥å¼•å¯¼å¼•æ“
try:
    from guidance.core.main_controller import CGEMainController
    CGE_AVAILABLE = True
    social_log.info("è®¤çŸ¥å¼•å¯¼å¼•æ“(CGE)æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    CGE_AVAILABLE = False
    social_log.warning(f"è®¤çŸ¥å¼•å¯¼å¼•æ“(CGE)æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥è‡ªå®šä¹‰å·¥å…·å‡½æ•°
from utils import (
    AsyncTqdmWrapper,
    generate_hybrid_user_profiles,
    export_data,
    load_user_contexts,
    export_user_context_data,
    load_cognition_space,
    get_individual_simulation_progress
)

# ä½¿ç”¨æ–°çš„ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from asce.social_agent.logging_system import get_logging_system
social_log = None
platform_log = None
agent_log = None
comprehensive_log = None

# åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
parser = argparse.ArgumentParser(description="Arguments for script.")  # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
parser.add_argument(
    "--config_path",
    type=str,
    help="Path to the YAML config file.",
    required=False,
    default="",
)  # æ·»åŠ é…ç½®æ–‡ä»¶è·¯å¾„å‚æ•°
parser.add_argument(
    "--seed",
    type=int,
    help="Random seed for reproducibility. If not provided, a random seed will be generated.",
    required=False,
    default=None,
)  # æ·»åŠ éšæœºç§å­å‚æ•°




# ç§»åˆ°æ¨¡å—åŒ–å‡½æ•°ä¸­é…ç½®
asyncio.get_event_loop().set_debug(False)


# ========== æ¨¡å—åŒ–å‡½æ•° ==========

def setup_logging():
    """
    è®¾ç½®ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
    ä½¿ç”¨æ–°çš„ASCEæ—¥å¿—ç³»ç»Ÿï¼Œåœ¨ /log ç›®å½•ä¸‹åˆ›å»ºä»¥æ—¶é—´å‘½åçš„å­ç›®å½•ï¼Œ
    åŒ…å«å››ä¸ªæ ¸å¿ƒæ—¥å¿—æ–‡ä»¶ï¼šagentã€platformã€simulationã€config
    
    è¿”å›:
        tuple: (log_dir, platform_log, agent_log, simulation_log)
    """
    global social_log, platform_log, agent_log, comprehensive_log
    
    # è·å–ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿ
    logging_system = get_logging_system()
    log_dir = logging_system.setup_session_logging()
    
    # è·å–å„ç§æ—¥å¿—å™¨
    platform_log = logging_system.get_platform_logger()
    agent_log = logging_system.get_agent_logger()
    simulation_log = logging_system.get_simulation_logger()
    
    # å‘åå…¼å®¹
    social_log = simulation_log
    comprehensive_log = simulation_log
    
    # é…ç½®åº“æ—¥å¿—è¾“å‡ºï¼Œç¦æ­¢ä¼ æ’­åˆ°æ§åˆ¶å°
    for logger_name in ["asce", "camel", "social.agent", "social.twitter"]:
        logger = logging.getLogger(logger_name)
        logger.propagate = False
    
    # æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
    logging_system.cleanup_old_logs()
    
    simulation_log.info(f"ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—ç›®å½•: {log_dir}")
    
    return log_dir, platform_log, agent_log, simulation_log


def load_and_save_config(config_path):
    """
    è¯»å–é…ç½®æ–‡ä»¶å¹¶ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿä¿å­˜é…ç½®ä¿¡æ¯
    
    å‚æ•°:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        dict: é…ç½®å‚æ•°å­—å…¸
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    # è¯»å–é…ç½®æ–‡ä»¶
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = safe_load(f)
    
    # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿä¿å­˜é…ç½®
    logging_system = get_logging_system()
    
    # å‡†å¤‡é…ç½®æ•°æ®
    config_data = {
        "é…ç½®æ–‡ä»¶è·¯å¾„": config_path,
        "é…ç½®åŠ è½½æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # æ·»åŠ å„ä¸ªé…ç½®èŠ‚
    for section_name, section_data in cfg.items():
        if isinstance(section_data, dict):
            config_data[f"--- {section_name.upper()} ---"] = ""
            for key, value in section_data.items():
                config_data[key] = value
        else:
            config_data[section_name] = section_data
    
    # ä¿å­˜é…ç½®å‚æ•°
    logging_system.save_config_parameters(config_data)
    
    return cfg


def setup_random_seed(args, config_path):
    """
    è®¾ç½®éšæœºç§å­
    
    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        int: è®¾ç½®çš„éšæœºç§å­å€¼
    """
    if args.seed is not None:
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æä¾›çš„ç§å­
        random_seed = args.seed
    else:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­æ˜¯å¦æœ‰éšæœºç§å­è®¾ç½®
        if os.path.exists(config_path):
            with open(config_path, "r") as f:
                cfg_temp = safe_load(f)
                simulation_params_temp = cfg_temp.get("simulation", {})
                config_seed = simulation_params_temp.get("random_seed")
                if config_seed is not None:
                    random_seed = config_seed
                else:
                    # ä½¿ç”¨å½“å‰æ—¶é—´ç”Ÿæˆéšæœºç§å­
                    random_seed = int(time.time()) % 10000
        else:
            # ä½¿ç”¨å½“å‰æ—¶é—´ç”Ÿæˆéšæœºç§å­
            random_seed = int(time.time()) % 10000
    
    # è®¾ç½®Pythonçš„randomæ¨¡å—ç§å­
    random.seed(random_seed)
    
    # å¦‚æœä½¿ç”¨äº†numpyï¼Œä¹Ÿè®¾ç½®numpyçš„éšæœºç§å­
    try:
        import numpy as np
        np.random.seed(random_seed)
        social_log.info("NumPyéšæœºç§å­å·²è®¾ç½®")
    except ImportError:
        social_log.info("NumPyä¸å¯ç”¨ï¼Œä»…è®¾ç½®Pythonéšæœºç§å­")
    
    # è®°å½•éšæœºç§å­åˆ°æ—¥å¿—
    print(f"{Fore.YELLOW}ä½¿ç”¨éšæœºç§å­: {random_seed}{Fore.RESET}")
    social_log.info(f"ä½¿ç”¨éšæœºç§å­: {random_seed}")
    
    return random_seed


def setup_cge_engine(guidance_engine_config, guidance_tasks_config, inference_configs, infra, db_path):
    """
    è®¾ç½®è®¤çŸ¥å¼•å¯¼å¼•æ“ï¼ˆCGEï¼‰
    
    å‚æ•°:
        guidance_engine_config: å¼•å¯¼å¼•æ“é…ç½®
        guidance_tasks_config: å¼•å¯¼ä»»åŠ¡é…ç½®
        inference_configs: æ¨ç†é…ç½®
        infra: å¹³å°å¯¹è±¡
        db_path: æ•°æ®åº“è·¯å¾„
        
    è¿”å›:
        CGEMainController: CGEæ§åˆ¶å™¨å¯¹è±¡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    cge_controller = None
    
    if CGE_AVAILABLE and guidance_engine_config and guidance_engine_config.get("enabled", False):
        try:
            # æ„å»ºå®Œæ•´é…ç½®
            full_config = {
                'guidance_engine': guidance_engine_config,
                'guidance_tasks': guidance_tasks_config or [],
                'inference': inference_configs
            }
            
            cge_controller = CGEMainController(full_config, infra, db_path)
            social_log.info(f"è®¤çŸ¥å¼•å¯¼å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼Œé…ç½®äº†{len(guidance_tasks_config)}ä¸ªå¼•å¯¼ä»»åŠ¡")
            
        except Exception as e:
            social_log.error(f"è®¤çŸ¥å¼•å¯¼å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            cge_controller = None
    elif guidance_engine_config and guidance_engine_config.get("enabled", False):
        social_log.warning("å¼•å¯¼å¼•æ“å·²å¯ç”¨ä½†CGEæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡å¼•å¯¼åŠŸèƒ½")
    
    return cge_controller


def initialize_user_data(params):
    """
    åˆå§‹åŒ–ç”¨æˆ·æ•°æ®
    
    å‚æ•°:
        params: å‚æ•°å­—å…¸
        
    è¿”å›:
        str: ç”¨æˆ·æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    if params['real_user_ratio'] == 1:
        user_path = params['real_user_path']
    else:
        # æ£€æŸ¥è¿™ä¸ªè·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(os.path.dirname(params['hybrid_user_profiles_path'])):
            os.makedirs(os.path.dirname(params['hybrid_user_profiles_path']), exist_ok=True)
            social_log.info(f"ç”Ÿæˆæ–°çš„æ··åˆç”¨æˆ·æ–‡ä»¶å¤¹: {os.path.dirname(params['hybrid_user_profiles_path'])}")
        if os.path.exists(params['hybrid_user_profiles_path']):
            user_path = params['hybrid_user_profiles_path']
            social_log.info(f"ä½¿ç”¨å·²å­˜åœ¨çš„æ··åˆç”¨æˆ·æ–‡ä»¶: {user_path}")
        else:
            user_path = generate_hybrid_user_profiles(
                params['real_user_ratio'], 
                params['num_agents'], 
                params['real_user_path'], 
                params['random_user_path'], 
                params['hybrid_user_profiles_path']
            )
            social_log.info(f"ç”Ÿæˆæ–°çš„æ··åˆç”¨æˆ·æ–‡ä»¶: {user_path}")
    
    # æ£€æŸ¥æ··åˆç”¨æˆ·æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(user_path):
        social_log.error(f"æ··åˆç”¨æˆ·æ–‡ä»¶ä¸å­˜åœ¨: {user_path}")
        raise FileNotFoundError(f"æ··åˆç”¨æˆ·æ–‡ä»¶ä¸å­˜åœ¨: {user_path}")
    
    print(f"hydrid_user_profiles_path: {params['hybrid_user_profiles_path']}")
    
    # æ£€æŸ¥æ··åˆç”¨æˆ·æ–‡ä»¶æ˜¯å¦ä¸ºç©º
    try:
        with open(user_path, "r", encoding="utf-8") as f:
            hybrid_users = json.load(f)
            if not hybrid_users:
                social_log.error(f"æ··åˆç”¨æˆ·æ–‡ä»¶ä¸ºç©º: {user_path}")
                raise ValueError(f"æ··åˆç”¨æˆ·æ–‡ä»¶ä¸ºç©º: {user_path}")
            social_log.info(f"æˆåŠŸåŠ è½½æ··åˆç”¨æˆ·æ–‡ä»¶ï¼ŒåŒ…å«{len(hybrid_users)}ä¸ªç”¨æˆ·")
    except json.JSONDecodeError:
        social_log.error(f"æ··åˆç”¨æˆ·æ–‡ä»¶æ ¼å¼é”™è¯¯: {user_path}")
        raise ValueError(f"æ··åˆç”¨æˆ·æ–‡ä»¶æ ¼å¼é”™è¯¯: {user_path}")
    
    return user_path


def load_post_data(params):
    """
    åŠ è½½å¸–å­æ•°æ®
    
    å‚æ•°:
        params: å‚æ•°å­—å…¸
        
    è¿”å›:
        list: å¸–å­æ•°æ®åˆ—è¡¨
    """
    data_format = params['data_format']
    post_path = params['post_path']
    total_news_articles = params['total_news_articles']
    round_post_num = params['round_post_num']
    num_timesteps = params['num_timesteps']
    
    if data_format == "reddit":
        social_log.info(f"Using Reddit data format from: {post_path}")
        with open(post_path, "r") as f:
            pairs = json.load(f)
    elif data_format == "twitter":
        social_log.info(f"Using Twitter data format from: {post_path}")
        with open(post_path, "r") as f:
            twitter_data = json.load(f)
        
        # å°†Twitteræ•°æ®è½¬æ¢ä¸ºä¸Redditæ ¼å¼ç›¸ä¼¼çš„ç»“æ„
        pairs = []
        for item in twitter_data:
            trigger_news = item.get("trigger_news", "")
            tweet_page = item.get("tweet_page", "")
            pair_item = {
                "RS": {
                    "title": "None",
                    "selftext": trigger_news
                },
                "RC_1": {
                    "body": tweet_page,
                    "group": "control"
                }
            }
            pairs.append(pair_item)
        social_log.info(f"Converted {len(pairs)} Twitter items to Reddit format")
    elif data_format == "twitter_raw":
        social_log.info(f"Using Twitter Raw data format from: {post_path}")
        with open(post_path, "r") as f:
            twitter_raw_data = json.load(f)
        
        pairs = []
        for item in twitter_raw_data:
            post_id = item.get("post_id", "")
            content = item.get("content", "")
            user_id = item.get("user_id", "")
            user_name = item.get("user_name", "")
            follower = item.get("follower", 0)
            location = item.get("location", "")
            timestamp = item.get("timestamp", "")
            likes = item.get("likes", 0)
            retweets = item.get("retweets", 0)
            views = item.get("views", 0)
            quotes = item.get("quotes", 0)
            
            formatted_post = f"""Tweet ID: {post_id}
            Author: {user_name} (@{user_id})
            Number of followers: {follower}
            Location: {location}
            Time: {timestamp}
            {content}
            â¤ï¸ {likes}  ğŸ”„ {retweets}  ğŸ‘ï¸ {views}  ğŸ’¬ {quotes}
            """
            
            pair_item = {
                "Original Post": {
                    "title": f"Tweet from {user_name}",
                    "text": formatted_post
                }
            }
            pairs.append(pair_item)
        social_log.info(f"Converted {len(pairs)} Twitter Raw items to Reddit format")
    else:
        raise ValueError(f"Unsupported data_format: {data_format}")
    
    # å¦‚æœæŒ‡å®šäº†total_news_articleså‚æ•°ï¼Œåˆ™é™åˆ¶ä½¿ç”¨çš„å¸–å­æ•°é‡
    if total_news_articles is not None and total_news_articles > 0:
        pairs = pairs[:total_news_articles]
        # ç¡®ä¿round_post_numä¸è¶…è¿‡å¯ç”¨å¸–å­æ•°é‡
        if round_post_num > len(pairs) // num_timesteps:
            round_post_num = max(1, len(pairs) // num_timesteps)
            social_log.info(f"è°ƒæ•´round_post_numä¸º{round_post_num}ï¼Œä»¥é€‚åº”æ€»å¸–å­æ•°é‡{len(pairs)}")
    
    return pairs


async def create_agent_graph(params, user_path, csv_path, twitter_channel, inference_channel, 
                           cognition_space_dict, action_space_prompt, 
                           is_openai_model, is_deepseek_model, deepseek_api_base, 
                           is_local_model, local_model_api_base, 
                           multi_api_handler, inference_configs):
    """
    åˆ›å»ºæ™ºèƒ½ä½“å›¾
    
    å‚æ•°:
        params: å‚æ•°å­—å…¸
        ... å…¶ä»–å‚æ•°
        
    è¿”å›:
        AgentGraph: æ™ºèƒ½ä½“å›¾å¯¹è±¡
    """
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¯æ§ç”¨æˆ·
    if not params['controllable_user']:
        raise ValueError("Uncontrollable user is not supported")
    
    # ç”Ÿæˆæ§åˆ¶ä»£ç†
    agent_graph, id_mapping = await gen_control_agents_with_data(
        params['data_name'],
        twitter_channel,
        control_user_num=2,
        cognition_space_dict=cognition_space_dict,
        action_space_prompt=action_space_prompt,
    )
    
    agent_graph = await generate_reddit_agents(
        params['data_name'],
        user_path,
        csv_path,
        twitter_channel,
        inference_channel,
        agent_graph,
        id_mapping,
        params['follow_post_agent'],
        params['mute_post_agent'],
        action_space_prompt,
        inference_configs["model_type"],
        is_openai_model,
        is_deepseek_model,
        deepseek_api_base,
        params['num_agents'],
        is_local_model,
        local_model_api_base,
        cognition_space_dict,
        multi_api_handler=multi_api_handler,
        max_concurrent_per_api=params['max_concurrent_per_api'],
        validate_cognitive_state=params['validate_cognitive_state'],
        max_retries=params['max_retries'],
        causal_method=params['causal_method'],
        causal_analysis_frequency=params['causal_analysis_frequency'],
        use_camel=params['use_camel'],
        max_tokens=params['max_tokens'],
        temperature=params['temperature'],
    )
    
    return agent_graph


def initialize_simulation_data(params, inference_configs, agent_graph):
    """
    åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®
    
    å‚æ•°:
        params: å‚æ•°å­—å…¸
        inference_configs: æ¨ç†é…ç½®
        agent_graph: æ™ºèƒ½ä½“å›¾
        
    è¿”å›:
        dict: æ¨¡æ‹Ÿæ•°æ®å­—å…¸
    """
    simulation_data = {
        "metadata": {
            "simulation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "timesteps": params['num_timesteps'],
            "model_type": inference_configs["model_type"]
        },
        "agents": [],
        "posts": [],
        "comments": [],
        "actions": []
    }
    
    # æ”¶é›†ä»£ç†ä¿¡æ¯
    for agent_id in range(agent_graph.get_num_nodes()):
        try:
            agent = agent_graph.get_agent(agent_id)
            agent_data = {
                "agent_id": agent_id,
                "username": agent.user_info.name,
                "bio": agent.user_info.description
            }
            simulation_data["agents"].append(agent_data)
        except Exception as e:
            social_log.error(f"Error collecting agent {agent_id} data: {e}")
    
    return simulation_data


def initialize_simulation_state(agent_graph, csv_path, params):
    """
    åˆå§‹åŒ–æ¨¡æ‹Ÿè¿è¡ŒçŠ¶æ€
    
    å‚æ•°:
        agent_graph: æ™ºèƒ½ä½“å›¾
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        params: å‚æ•°å­—å…¸
        
    è¿”å›:
        tuple: åˆå§‹åŒ–çš„çŠ¶æ€å˜é‡
    """
    # ç»´æŠ¤æ´»è·ƒç”¨æˆ·æ± å’Œæ‰€æœ‰éæ§åˆ¶ç”¨æˆ·çš„è®°å½•
    active_users_pool = set()
    all_non_controllable_agents = []
    
    # è·Ÿè¸ªæ¨¡æ‹ŸçŠ¶æ€
    simulation_success = True
    
    start_time_0 = datetime.now()
    agents = list(agent_graph.get_agents())
    
    # è¿‡æ»¤å‡ºéå¯æ§ç”¨æˆ·
    for agent_id, agent in agents:
        if not agent.user_info.is_controllable:
            all_non_controllable_agents.append((agent_id, agent))
        agent.csv_path = csv_path
    
    # è®¡ç®—æ¯è½®åº”è¯¥æ¿€æ´»çš„å›ºå®šç”¨æˆ·æ•°
    fixed_num_users_to_activate = int(len(all_non_controllable_agents) * params['activate_prob'])
    social_log.info(f"æ¯è½®å°†å›ºå®šæ¿€æ´»{fixed_num_users_to_activate}ä¸ªç”¨æˆ·ï¼ˆå…±{len(all_non_controllable_agents)}ä¸ªéæ§åˆ¶ç”¨æˆ·ï¼‰")
    
    progress_bar = AsyncTqdmWrapper(
        total=params['num_timesteps'], 
        desc=f"ASCEæ¨¡æ‹Ÿè¿›åº¦,å‚ä¸æ™ºèƒ½ä½“æ•°:{len(all_non_controllable_agents)},æ¿€æ´»ç‡:{params['activate_prob']}[0/{params['num_timesteps']}è½®]:\n", 
        colour="green"
    )
    
    # åˆ›å»ºä¸€ä¸ªå…¨å±€å­—å…¸ç”¨äºå­˜å‚¨æ‰€æœ‰ç”¨æˆ·çš„è®¤çŸ¥ç”»åƒ
    users_cognitive_profile_dict = {}
    
    # ç”¨äºè·Ÿè¸ªå“åº”è¿›åº¦
    completed_responses = 0
    response_count_by_round = {}
    
    # å®‰å…¨æ£€æŸ¥
    if len(all_non_controllable_agents) == 0:
        social_log.error("æ²¡æœ‰å¯ç”¨çš„éæ§åˆ¶æ™ºèƒ½ä½“ï¼Œæ— æ³•è¿›è¡Œæ¨¡æ‹Ÿ")
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„éæ§åˆ¶æ™ºèƒ½ä½“")
    
    return (
        active_users_pool, all_non_controllable_agents, fixed_num_users_to_activate,
        simulation_success, progress_bar, users_cognitive_profile_dict,
        completed_responses, response_count_by_round, start_time_0
    )


def extract_simulation_parameters(data_params, simulation_params, inference_configs):
    """
    æå–å’Œæ•´ç†æ¨¡æ‹Ÿå‚æ•°
    
    å‚æ•°:
        data_params: æ•°æ®å‚æ•°
        simulation_params: æ¨¡æ‹Ÿå‚æ•°
        inference_configs: æ¨ç†é…ç½®
        
    è¿”å›:
        dict: æ•´ç†åçš„å‚æ•°å­—å…¸
    """
    params = {
        # æ•°æ®ç›¸å…³å‚æ•°
        'data_name': data_params["data_name"],
        'db_path': data_params["db_path"],
        'real_user_path': data_params["real_user_path"],
        'random_user_path': data_params["random_user_path"],
        'hybrid_user_profiles_path': data_params["hybrid_user_profiles_path"],
        'post_path': data_params["post_path"],
        'action_space_file_path': data_params["normal_space_file_path"],
        'csv_path': data_params["csv_path"],
        'cognitive_space_path': data_params["cognitive_space_path"],
        
        # æ¨¡æ‹Ÿç›¸å…³å‚æ•°
        'recsys_type': simulation_params["recsys_type"],
        'controllable_user': simulation_params["controllable_user"],
        'allow_self_rating': simulation_params["allow_self_rating"],
        'show_score': simulation_params["show_score"],
        'max_rec_post_len': simulation_params["max_rec_post_len"],
        'refresh_rec_post_count': simulation_params["refresh_rec_post_count"],
        'activate_prob': simulation_params["activate_prob"],
        'data_format': simulation_params["data_format"],
        'max_concurrent_per_api': simulation_params["max_concurrent_per_api"],
        'validate_cognitive_state': simulation_params["validate_cognitive_state"],
        'max_retries': simulation_params["max_retries"],
        'use_camel': simulation_params.get("use_camel", False),
        'total_news_articles': simulation_params["total_news_articles"],
        'round_post_num': simulation_params["round_post_num"],
        'num_timesteps': simulation_params["num_timesteps"],
        'num_agents': simulation_params["num_agents"],
        'follow_post_agent': simulation_params["follow_post_agent"],
        'mute_post_agent': simulation_params["mute_post_agent"],
        'real_user_ratio': simulation_params["real_user_ratio"],
        'clock_factor': simulation_params["clock_factor"],
        'init_post_score': simulation_params["init_post_score"],
        'max_visible_comments': simulation_params.get("max_visible_comments", 5),
        'max_total_comments': simulation_params.get("max_total_comments", 10),
        'save_mode': simulation_params.get("save_mode", "db"),
        'num_historical_memory': simulation_params.get("num_historical_memory", 2),
        'prompt_mode': simulation_params["prompt_mode"],
        'think_mode': simulation_params["think_mode"],
        'causal_method': simulation_params.get("causal_method", "dbn_custom"),
        'causal_analysis_frequency': simulation_params.get("causal_analysis_frequency", 2),
        
        # APIç›¸å…³å‚æ•°
        'model_type': inference_configs["model_type"],
        'is_openai_model': inference_configs["is_openai_model"],
        'is_deepseek_model': inference_configs["is_deepseek_model"],
        'is_local_model': inference_configs["is_local_model"],
        'local_model_api_base': inference_configs["local_model_api_base"],
        'max_tokens': inference_configs["max_tokens"],
        'temperature': inference_configs["temperature"],
    }
    
    return params


def setup_simulation_environment(params, random_seed):
    """
    è®¾ç½®æ¨¡æ‹Ÿç¯å¢ƒ
    
    å‚æ•°:
        params: å‚æ•°å­—å…¸
        random_seed: éšæœºç§å­
        
    è¿”å›:
        tuple: (infra, twitter_channel, cognition_space_dict, multi_api_handler, db_path, csv_path, think_csv_path)
    """
    # è®¾ç½®éšæœºç§å­
    if random_seed is not None:
        social_log.info(f"Setting random seed in simulation environment: {random_seed}")
        random.seed(random_seed)
        try:
            import numpy as np
            np.random.seed(random_seed)
            social_log.info("NumPy random seed also set")
        except ImportError:
            social_log.info("NumPy not available, only Python random seed set")
    
    social_log.info(f"Using data format: {params['data_format']}")
    social_log.info(f"Using post data path: {params['post_path']}")
    
    # åŠ è½½è®¤çŸ¥ç©ºé—´
    cognition_space_dict = load_cognition_space(params['data_name'], params['cognitive_space_path'])
    print(json.dumps(cognition_space_dict, indent=2))
    
    # å¦‚æœæ•°æ®åº“æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åˆ é™¤å®ƒ
    if os.path.exists(params['db_path']):
        os.remove(params['db_path'])
    
    # è®¾ç½®æ¨¡æ‹Ÿå¼€å§‹æ—¶é—´å’Œæ—¶é’Ÿ
    start_time = datetime(2025, 4, 9, 10, 0)
    clock = Clock(k=params['clock_factor'])
    twitter_channel = Channel()
    
    # è¯»å–åŠ¨ä½œç©ºé—´æç¤ºæ–‡ä»¶
    with open(params['action_space_file_path'], "r", encoding="utf-8") as file:
        action_space_prompt = file.read()
    
    # ä¸ºæ•°æ®åº“æ–‡ä»¶åæ·»åŠ æ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¸ºè‡ªå®šä¹‰æ•°æ®åº“è·¯å¾„æ·»åŠ æ—¶é—´æˆ³
    db_dir = os.path.dirname(params['db_path'])
    db_filename = os.path.basename(params['db_path']).split('.')[0]
    db_ext = os.path.basename(params['db_path']).split('.')[-1]
    db_path = os.path.join(db_dir, f"{db_filename}_{timestamp}.{db_ext}")
    
    csv_dir = os.path.dirname(params['csv_path'])
    csv_filename = os.path.basename(params['csv_path']).split('.')[0]
    csv_path = os.path.join(csv_dir, f"user_action_{csv_filename}_{timestamp}.csv")
    think_csv_path = os.path.join(csv_dir, f"user_action_think_{csv_filename}_{timestamp}.csv")
    
    # åˆ›å»ºæ•°æ®åº“ç›®å½•
    if not os.path.exists(db_dir):
        os.makedirs(db_dir, exist_ok=True)
        social_log.info(f"Created database directory: {db_dir}")
    
    # åˆ›å»ºCSVè¾“å‡ºç›®å½•
    if not os.path.exists(csv_dir):
        os.makedirs(csv_dir, exist_ok=True)
        social_log.info(f"Created CSV directory: {csv_dir}")
    
    # åˆ›å»ºå¹³å°å¯¹è±¡
    infra = Platform(
        db_path,
        twitter_channel,
        clock,
        start_time,
        allow_self_rating=params['allow_self_rating'],
        show_score=params['show_score'],
        recsys_type=params['recsys_type'],
        max_rec_post_len=params['max_rec_post_len'],
        refresh_rec_post_count=params['refresh_rec_post_count'],
        activate_prob=params['activate_prob'],
        max_visible_comments=params['max_visible_comments'],
        max_total_comments=params['max_total_comments'],
    )
    
    # åœ¨å¹³å°å¯åŠ¨ä¹‹å‰åˆå§‹åŒ–ç¯å¢ƒå˜é‡ï¼Œé¿å…SANDBOX_TIMEæœªå®šä¹‰çš„é”™è¯¯
    os.environ["TIME_STAMP"] = "0"
    os.environ["SANDBOX_TIME"] = "0"
    social_log.info("å·²åˆå§‹åŒ–ç¯å¢ƒå˜é‡ TIME_STAMP å’Œ SANDBOX_TIME ä¸º 0")
    
    # è®¾ç½®å¤š API å¤„ç†å™¨
    multi_api_handler = None
    if params['is_local_model'] and params['local_model_api_base'] and "," in params['local_model_api_base']:
        # åˆ†å‰²API URL
        api_urls = [url.strip() for url in params['local_model_api_base'].split(",")]
        social_log.info(f"æ£€æµ‹åˆ°å¤šAPIé…ç½®ï¼ŒAPIæ•°é‡: {len(api_urls)}")
        social_log.info(f"APIåˆ—è¡¨: {api_urls}")
        
        # å¯¼å…¥MultiApiHandler
        from asce.social_agent.api_handler import MultiApiHandler
        
        # åˆ›å»ºMultiApiHandlerå®ä¾‹
        multi_api_handler = MultiApiHandler(
            api_urls=api_urls,
            max_concurrent_per_api=params['max_concurrent_per_api'],
            max_retries=params['max_retries'],
            validate_cognitive_state=params['validate_cognitive_state']
        )
        social_log.info(f"å·²åˆ›å»ºMultiApiHandlerï¼Œæœ€å¤§å¹¶å‘æ•°: {params['max_concurrent_per_api']}ï¼Œæœ€å¤§é‡è¯•æ•°: {params['max_retries']}")
        social_log.info(f"è®¤çŸ¥çŠ¶æ€éªŒè¯: {'å·²å¯ç”¨' if params['validate_cognitive_state'] else 'å·²ç¦ç”¨'}")
    else:
        social_log.info(f"ä½¿ç”¨å•ä¸€APIé…ç½®: {params['local_model_api_base']}")
    
    return infra, twitter_channel, cognition_space_dict, multi_api_handler, db_path, csv_path, think_csv_path, action_space_prompt


# ä¸»è¦è¿è¡Œå‡½æ•°ï¼Œä½¿ç”¨asyncå…³é”®å­—å®šä¹‰ä¸ºå¼‚æ­¥å‡½æ•°
async def normal_running(data_params=None,
                    model_configs=None,
                    inference_configs=None,
                    simulation_params = None,
                    guidance_engine_config=None,
                    guidance_tasks_config=None,
                    random_seed=None):

    # æå–å’Œæ•´ç†å‚æ•°
    params = extract_simulation_parameters(data_params, simulation_params, inference_configs)
    
    # è®¾ç½®æ¨¡æ‹Ÿç¯å¢ƒ
    infra, twitter_channel, cognition_space_dict, multi_api_handler, db_path, csv_path, think_csv_path, action_space_prompt = setup_simulation_environment(params, random_seed)




    # åˆ›å»ºå¹¶å¯åŠ¨å¹³å°è¿è¡Œä»»åŠ¡
    twitter_task = asyncio.create_task(infra.running())
    inference_channel = Channel()  # åˆ›å»ºæ¨ç†é€šé“å¯¹è±¡
    
    # åˆå§‹åŒ–è®¤çŸ¥å¼•å¯¼å¼•æ“ (CGE)
    cge_controller = setup_cge_engine(guidance_engine_config, guidance_tasks_config, inference_configs, infra, db_path)
    
    # è®¾ç½®æ¨¡å‹é…ç½®ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
    is_openai_model = params['is_openai_model']
    is_deepseek_model = params['is_deepseek_model'] 
    is_local_model = params['is_local_model']
    deepseek_api_base = inference_configs.get("deepseek_api_base")
    local_model_api_base = params['local_model_api_base']
    
    # å¦‚æœæœªé…ç½®æ¨¡å‹ç›¸å…³å‚æ•°
    if not (is_openai_model or is_deepseek_model or is_local_model):
        social_log.warning("æœªé…ç½®ä»»ä½•æ¨¡å‹ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
        is_local_model = True

    # åˆå§‹åŒ–ç”¨æˆ·æ•°æ®
    user_path = initialize_user_data(params)
    causal_json_file_path = None  # é»˜è®¤å€¼


    # ç”Ÿæˆæ™ºèƒ½ä½“
    agent_graph = await create_agent_graph(params, user_path, csv_path, twitter_channel, inference_channel, 
                                         cognition_space_dict, action_space_prompt, 
                                         is_openai_model, is_deepseek_model, deepseek_api_base, 
                                         is_local_model, local_model_api_base, 
                                         multi_api_handler, inference_configs)

    # åŠ è½½å¸–å­æ•°æ®
    pairs = load_post_data(params)

    # åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®å’ŒçŠ¶æ€
    simulation_data = initialize_simulation_data(params, inference_configs, agent_graph)

    # åˆå§‹åŒ–æ¨¡æ‹Ÿè¿è¡ŒçŠ¶æ€
    (
        active_users_pool, all_non_controllable_agents, fixed_num_users_to_activate, 
        simulation_success, progress_bar, users_cognitive_profile_dict, 
        completed_responses, response_count_by_round, start_time_0
    ) = initialize_simulation_state(agent_graph, csv_path, params)


    # å¼€å§‹æ—¶é—´æ­¥å¾ªç¯ - æ¨¡æ‹Ÿä¸»å¾ªç¯
    for timestep in range(0, params['num_timesteps'] + 1):

        # é©±åŠ¨è®¤çŸ¥å¼•å¯¼å¼•æ“
        if cge_controller:
            try:
                cge_controller.advance_timestep(timestep)
                social_log.debug(f"æ—¶é—´æ­¥ {timestep}: CGEå¼•å¯¼å¼•æ“å¤„ç†å®Œæˆ")
            except Exception as e:
                social_log.error(f"æ—¶é—´æ­¥ {timestep}: CGEå¼•å¯¼å¼•æ“å¤„ç†å¤±è´¥: {e}")

        if timestep == 0:
            # ---------- åˆå§‹åŒ–é˜¶æ®µ ----------
            social_log.info("======= åˆå§‹åŒ–é˜¶æ®µ(ç¬¬0è½®) =======")
            init_tasks = []
            # ä¸ºæ‰€æœ‰éå¯æ§ç”¨æˆ·æ„å»ºåˆå§‹åŒ–ä»»åŠ¡
            for agent_id, agent in all_non_controllable_agents:
                agent.step_counter = timestep
                agent.think_mode = params['think_mode']
                agent.num_historical_memory = params['num_historical_memory']
                init_tasks.append(agent.initialize_cognitive_profile())
                init_tasks.append(agent.init_save_user_information())

            # å¹¶å‘æ‰§è¡Œåˆå§‹åŒ–
            if init_tasks:
                social_log.info(f"å¼€å§‹ä¸º{len(init_tasks)}ä¸ªç”¨æˆ·æ‰§è¡Œè®¤çŸ¥æ¡£æ¡ˆåˆå§‹åŒ–")
                await asyncio.gather(*init_tasks)
                social_log.info("å®Œæˆè®¤çŸ¥æ¡£æ¡ˆåˆå§‹åŒ–")

                # ä¿å­˜æ‰€æœ‰ç”¨æˆ·çš„è®¤çŸ¥ç”»åƒåˆ°å…¨å±€å­—å…¸
                for agent_id, agent in all_non_controllable_agents:
                    # ç¡®ä¿cognitive_profileä¸ä¸ºNoneï¼Œå¦‚æœä¸ºNoneåˆ™åˆå§‹åŒ–ä¸ºé»˜è®¤è®¤çŸ¥æ¡£æ¡ˆ
                    if not hasattr(agent, 'cognitive_profile') or agent.cognitive_profile is None:
                        social_log.warning(f"ç”¨æˆ·{agent_id}çš„è®¤çŸ¥æ¡£æ¡ˆä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤è®¤çŸ¥æ¡£æ¡ˆ")
                        # è®¾ç½®é»˜è®¤è®¤çŸ¥æ¡£æ¡ˆ
                        await agent.initialize_cognitive_profile()

                    users_cognitive_profile_dict[agent_id] = agent.cognitive_profile.copy()
                social_log.info(f"å·²å°†{len(users_cognitive_profile_dict)}ä¸ªç”¨æˆ·çš„è®¤çŸ¥ç”»åƒä¿å­˜åˆ°å…¨å±€å­—å…¸")
                social_log.info("==== ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“çš„åˆå§‹çŠ¶æ€ï¼ˆè½®æ¬¡ä¸º0ï¼‰====")
                await save_user_actions_to_csv(all_non_controllable_agents, csv_path, think_csv_path, timestep, include_initial_state=True)
                social_log.info("åˆå§‹çŠ¶æ€ä¿å­˜å®Œæˆ")
        else:
            try:
                os.environ["TIME_STAMP"] = str(timestep)
                os.environ["SANDBOX_TIME"] = str(timestep)  # æ·»åŠ SANDBOX_TIMEç¯å¢ƒå˜é‡

                # æ›´æ–°è¿›åº¦æ¡
                await progress_bar.update(1)
                progress_bar.set_description(f"æ¨¡æ‹Ÿè¿›åº¦ ({timestep}/{params['num_timesteps']}è½®)")

                social_log.info(f"======= æ—¶é—´æ­¥ {timestep}/{params['num_timesteps']} =======")

                # æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“çš„è½®æ¬¡è®¡æ•°å™¨å’Œè®¤çŸ¥ç”»åƒå­—å…¸å¼•ç”¨
                for agent_id, agent in agent_graph.get_agents():
                    agent.users_cognitive_profile_dict = users_cognitive_profile_dict
                    agent.step_counter = timestep
                # è·å–å‘å¸–ä»£ç†/è¯„åˆ†ä»£ç†
                post_agent = agent_graph.get_agent(0)
                rate_agent = agent_graph.get_agent(1)

                await export_data(post_agent, rate_agent, pairs, timestep, params['round_post_num'], params['data_format'], params['init_post_score'])
                await infra.update_rec_table()
                social_log.info("æ›´æ–°æ¨èè¡¨å®Œæˆ")
                # æ¸…ç©ºä¸Šä¸€è½®çš„æ´»è·ƒç”¨æˆ·æ± 
                active_users_pool.clear()
                # å†æ¬¡æ ¡éªŒéæ§åˆ¶ç”¨æˆ·åˆ—è¡¨
                valid_agents = []
                for aid, agent in all_non_controllable_agents:
                    if agent and agent.user_info:
                        valid_agents.append((aid, agent))
                all_non_controllable_agents = valid_agents

                if not all_non_controllable_agents:
                    social_log.error(f"æ—¶é—´æ­¥{timestep}ï¼šæ‰€æœ‰éæ§åˆ¶ç”¨æˆ·éƒ½æ— æ•ˆï¼")
                    simulation_success = False
                    break

                # éšæœºæŒ‘é€‰è¦æ¿€æ´»çš„ä»£ç†
                selected_agents = random.sample(all_non_controllable_agents, min(fixed_num_users_to_activate, len(all_non_controllable_agents)))
                for aid, _ in selected_agents:
                    active_users_pool.add(aid)

                    # è®°å½•æ¿€æ´»åˆ—è¡¨
                with open("active_users_log.txt", "a", encoding="utf-8") as f:
                    f.write(f"æ—¶é—´æ­¥ {timestep}:\n")
                    f.write(f"æ€»æ¿€æ´»ç”¨æˆ·æ•°: {len(active_users_pool)}\n")
                    f.write(f"æ¿€æ´»ç”¨æˆ·ID: {sorted(list(active_users_pool))}\n")
                    f.write("-" * 50 + "\n")

                social_log.info(f"æ—¶é—´æ­¥{timestep}æ¿€æ´»äº†{len(active_users_pool)}ä¸ªç”¨æˆ·: {sorted(list(active_users_pool))}")

                # è®©æ¿€æ´»çš„ç”¨æˆ·æ‰§è¡ŒåŠ¨ä½œ
                tasks = []
                causal_json_file_path = params['data_name'] + "_causal.json"  # ä½¿ç”¨å‚æ•°ä¸­çš„æ•°æ®åç§°
                for aid, agent in selected_agents:
                    agent.causal_json_file_path = params['data_name'] + "_causal.json"
                    tasks.append(agent.perform_action_by_llm(save_mode=params['save_mode']))
                random.shuffle(tasks)

                if tasks:
                    social_log.info(f"æ‰§è¡Œ{len(tasks)}ä¸ªç”¨æˆ·æ“ä½œä»»åŠ¡")
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    # æ£€æŸ¥æ‰§è¡Œç»“æœ
                    success_count = sum(1 for r in results if r is not None and not isinstance(r, Exception))
                    error_count = sum(1 for r in results if isinstance(r, Exception))
                    social_log.info(f"ç”¨æˆ·æ“ä½œæ‰§è¡Œç»“æœ: æˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{error_count}ä¸ª")
                else:
                    social_log.warning("æ²¡æœ‰ç”¨æˆ·è¢«æ¿€æ´»ï¼Œè·³è¿‡æ‰§è¡Œç”¨æˆ·æ“ä½œ")

                active_agents = selected_agents
                active_ids = {aid for aid, _ in selected_agents}
                non_active_agents = [(aid, agent) for aid, agent in all_non_controllable_agents if aid not in active_ids]

                # ä¿å­˜ç”¨æˆ·è¡Œä¸ºæ•°æ®
                if params['save_mode'] == "db":
                    # çº¯æ•°æ®åº“ä¿å­˜æ¨¡å¼ï¼šæ‰€æœ‰æ•°æ®éƒ½ä¿å­˜åˆ°æ•°æ®åº“
                    db_save_tasks = []
                    for aid, agent in active_agents:
                        agent.is_active = True
                        db_save_tasks.append(agent.save_user_action_dict(save_mode="db"))
                    for aid, agent in non_active_agents:
                        agent.is_active = False
                        db_save_tasks.append(agent.save_user_action_dict(save_mode="db"))

                    # ç­‰å¾…æ•°æ®åº“ä¿å­˜å®Œæˆ
                    if db_save_tasks:
                        await asyncio.gather(*db_save_tasks)
                        social_log.info(f"å·²æˆåŠŸå°†{len(db_save_tasks)}ä¸ªç”¨æˆ·çš„è¡Œä¸ºæ•°æ®ä¿å­˜åˆ°æ•°æ®åº“")

                elif params['save_mode'] == "csv":
                    # çº¯CSVä¿å­˜æ¨¡å¼ï¼šåªå°†è®¤çŸ¥çŠ¶æ€ä¿å­˜åˆ°CSVæ–‡ä»¶
                    csv_save_tasks = []
                    # å…ˆæ›´æ–°æ¯ä¸ªæ™ºèƒ½ä½“çš„user_action_dict
                    for aid, agent in active_agents:
                        agent.is_active = True
                        csv_save_tasks.append(agent.save_user_action_dict(save_mode="csv"))
                    for aid, agent in non_active_agents:
                        agent.is_active = False
                        csv_save_tasks.append(agent.save_user_action_dict(save_mode="csv"))

                    # ç­‰å¾…æ‰€æœ‰æ™ºèƒ½ä½“æ›´æ–°å®Œæˆuser_action_dict
                    if csv_save_tasks:
                        await asyncio.gather(*csv_save_tasks)

                    # ä¿å­˜åˆ°CSVæ–‡ä»¶
                    await save_user_actions_to_csv(all_non_controllable_agents, csv_path, think_csv_path, timestep, include_initial_state=False)
                    social_log.info(f"å·²æˆåŠŸå°†ç”¨æˆ·çš„è®¤çŸ¥çŠ¶æ€ä¿å­˜åˆ°CSVæ–‡ä»¶")

                elif params['save_mode'] == "both":
                    # æ··åˆä¿å­˜æ¨¡å¼ï¼š
                    # 1. è¡Œä¸ºæ•°æ®ï¼ˆå¦‚å…³æ³¨ã€ç‚¹èµç­‰ï¼‰ä¿å­˜åˆ°æ•°æ®åº“
                    # 2. è®¤çŸ¥çŠ¶æ€åªä¿å­˜åœ¨CSVæ–‡ä»¶ä¸­

                    # å…ˆå¤„ç†æ•°æ®åº“ä¿å­˜ï¼ˆä¸åŒ…å«è®¤çŸ¥çŠ¶æ€ï¼‰
                    db_save_tasks = []
                    for aid, agent in active_agents:
                        agent.is_active = True
                        db_save_tasks.append(agent.save_user_action_dict(save_mode="both"))
                    for aid, agent in non_active_agents:
                        agent.is_active = False
                        db_save_tasks.append(agent.save_user_action_dict(save_mode="both"))

                    # ç­‰å¾…æ•°æ®åº“ä¿å­˜å®Œæˆ
                    if db_save_tasks:
                        await asyncio.gather(*db_save_tasks)
                        social_log.info(f"å·²æˆåŠŸå°†{len(db_save_tasks)}ä¸ªç”¨æˆ·çš„è¡Œä¸ºæ•°æ®ï¼ˆä¸å«è®¤çŸ¥çŠ¶æ€ï¼‰ä¿å­˜åˆ°æ•°æ®åº“")

                    # å†å¤„ç†CSVä¿å­˜ï¼ˆåŒ…å«è®¤çŸ¥çŠ¶æ€ï¼‰
                    csv_save_tasks = []
                    # å…ˆæ›´æ–°æ¯ä¸ªæ™ºèƒ½ä½“çš„user_action_dict
                    for aid, agent in active_agents:
                        agent.is_active = True
                        csv_save_tasks.append(agent.save_user_action_dict(save_mode="csv"))
                    for aid, agent in non_active_agents:
                        agent.is_active = False
                        csv_save_tasks.append(agent.save_user_action_dict(save_mode="csv"))

                    # ç­‰å¾…æ‰€æœ‰æ™ºèƒ½ä½“æ›´æ–°å®Œæˆuser_action_dict
                    if csv_save_tasks:
                        await asyncio.gather(*csv_save_tasks)

                    # ä¿å­˜åˆ°CSVæ–‡ä»¶
                    await save_user_actions_to_csv(all_non_controllable_agents, csv_path, think_csv_path, timestep, include_initial_state=False)
                    social_log.info(f"å·²æˆåŠŸå°†ç”¨æˆ·çš„è®¤çŸ¥çŠ¶æ€ä¿å­˜åˆ°CSVæ–‡ä»¶")

                else:
                    social_log.warning(f"æœªçŸ¥çš„ä¿å­˜æ¨¡å¼: {params['save_mode']}ï¼Œè·³è¿‡ä¿å­˜ç”¨æˆ·è¡Œä¸ºæ•°æ®")


                # # ä¿å­˜è®¤çŸ¥çŠ¶æ€å’Œå› æœåˆ†ææ•°æ®
                # memory_tasks = []
                # for aid, agent in active_agents:
                #     memory_tasks.append(agent.save_cognitive_state_to_json(causal_json_file_path))

                # for aid, agent in non_active_agents:
                #     #memory_tasks.append(agent.save_memory_data(timestep, False))
                #     memory_tasks.append(agent.save_cognitive_state_to_json(causal_json_file_path))

                # æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“çš„è½®æ¬¡è®¡æ•°å™¨å’Œè®¤çŸ¥ç”»åƒå­—å…¸å¼•ç”¨

                # if memory_tasks:
                #     try:
                #         await asyncio.gather(*memory_tasks)
                #         social_log.info(f"å·²ä¿å­˜{len(memory_tasks)}ä¸ªç”¨æˆ·çš„è®°å¿†æ•°æ®")
                #     except Exception as e:
                #         social_log.error(f"ä¿å­˜è®°å¿†æ•°æ®æ—¶å‡ºé”™: {e}")

                for agent_id, agent in agent_graph.get_agents():
                    users_cognitive_profile_dict = agent.users_cognitive_profile_dict

                # å¦‚æœæ˜¯ç¬¬1è½®ï¼Œé‡æ–°è®¡ç®—æ—¶é’Ÿå› å­
                if timestep == 1:
                    time_difference = datetime.now() - start_time_0
                    two_hours_in_seconds = timedelta(hours=2).total_seconds()
                    clock_factor = two_hours_in_seconds / time_difference.total_seconds()
                    # æ³¨æ„: åœ¨æ¨¡å—åŒ–ç‰ˆæœ¬ä¸­ï¼Œclockå¯¹è±¡éœ€è¦ä»ç¯å¢ƒä¸­è·å–
                    # è¿™é‡Œçš„clockå¯¹è±¡åœ¨setup_simulation_environmentä¸­åˆ›å»º
                    # ä½†ç”±äºåŸå§‹ä»£ç çš„é™åˆ¶ï¼Œè¿™é‡Œä¿ç•™åŸæœ‰é€»è¾‘ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–
                    social_log.info(f"clock_factoré‡è®¾ä¸º: {clock_factor}")
                    social_log.warning("æ³¨æ„: clockå¯¹è±¡çš„æ›´æ–°åœ¨æ¨¡å—åŒ–ç‰ˆæœ¬ä¸­å¯èƒ½éœ€è¦è°ƒæ•´")

                # æµ‹è¯•æ•°æ®åº“ä¸­æœ¬è½®è¡Œä¸ºæ•°æ®
                db_conn = sqlite3.connect(db_path)
                cursor = db_conn.cursor()
                # è¿™é‡Œå¯ä»¥æ‰§è¡ŒæŸ¥è¯¢æˆ–åˆ«çš„æ£€æŸ¥
                db_conn.close()

                # ç¡®ä¿æ‰€æœ‰ç¼“å­˜æ•°æ®éƒ½è¢«ä¿å­˜åˆ°æ•°æ®åº“
                await infra._flush_all_caches()

                # è®°å½•æœ¬è½®å“åº”æ•°
                response_count_by_round[timestep] = len(active_users_pool)
                social_log.info(f"æ—¶é—´æ­¥{timestep}å®Œæˆäº†{len(active_users_pool)}ä¸ªå“åº”ï¼Œç´¯è®¡å®Œæˆ{completed_responses}/{len(active_users_pool)}")

            except Exception as e:
                social_log.error(f"æ¨¡æ‹Ÿè¿è¡Œå‡ºé”™(ç¬¬{timestep}è½®): {e}")
                simulation_success = False
                break

    # æ¨¡æ‹Ÿç»“æŸæ—¶ï¼Œç¡®ä¿æ‰€æœ‰ç¼“å­˜æ•°æ®éƒ½è¢«ä¿å­˜åˆ°æ•°æ®åº“
    try:
        await infra._flush_all_caches()
        social_log.info("æ¨¡æ‹Ÿç»“æŸæ—¶æˆåŠŸå°†æ‰€æœ‰ç¼“å­˜æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“")
    except Exception as e:
        social_log.error(f"æ¨¡æ‹Ÿç»“æŸæ—¶ä¿å­˜ç¼“å­˜æ•°æ®å‡ºé”™: {e}")

    social_log.info(f"æ¨¡æ‹Ÿæ•°æ®å·²ä¿å­˜ä¸ºæ•°æ®åº“: {db_path}")
    social_log.info("==== æ¨¡æ‹Ÿå®ŒæˆçŠ¶æ€æŠ¥å‘Š ====")
    social_log.info(f"æ‰§è¡Œæ—¶é—´æ­¥: {params['num_timesteps']}")
    social_log.info(f"æ¨¡æ‹ŸçŠ¶æ€: {'æˆåŠŸ' if simulation_success else 'æœ‰é”™è¯¯'}")

    try:
        # è¿æ¥æ•°æ®åº“è·å–æœ€ç»ˆçŠ¶æ€
        db_conn = sqlite3.connect(db_path)
        cursor = db_conn.cursor()

        # æ£€æŸ¥thinkè¡¨ä¸­æœ€å¤§çš„æ—¶é—´æ­¥å’Œæ¯ä¸ªæ—¶é—´æ­¥çš„è®°å½•æ•°
        cursor.execute("SELECT MAX(step_number) FROM think")
        max_step = cursor.fetchone()[0]
        social_log.info(f"æ•°æ®åº“ä¸­æœ€å¤§æ—¶é—´æ­¥: {max_step}")

        cursor.execute("SELECT step_number, COUNT(*) FROM think GROUP BY step_number ORDER BY step_number")
        step_counts = cursor.fetchall()
        social_log.info(f"å„æ—¶é—´æ­¥è®°å½•æ•°: {step_counts}")

        # æœ€åå…³é—­è¿æ¥
        db_conn.close()
    except Exception as e:
        social_log.error(f"ç”ŸæˆçŠ¶æ€æŠ¥å‘Šæ—¶å‡ºé”™: {e}")

    # ç»ˆæ­¢å¹³å°ä»»åŠ¡
    twitter_task.cancel()

    try:
        await twitter_task
    except asyncio.CancelledError:
        pass

    # å…³é—­è¿›åº¦æ¡
    await progress_bar.close()

    print(f"\n{Fore.GREEN}===== æ¨¡æ‹Ÿå®Œæˆ ====={Fore.RESET}")
    print(f"æ•°æ®åº“æ–‡ä»¶: {db_path}")
    print(f"CSVæ–‡ä»¶: {csv_path}")
    print(f"CSV THINKæ–‡ä»¶: {think_csv_path}")


    # æ‰“å°å“åº”è´¨é‡ç»Ÿè®¡ä¿¡æ¯
    try:
        from asce.social_agent.utils.response_quality_utils import print_response_quality_stats
        print_response_quality_stats(save_to_file=True)
        social_log.info("å·²æ‰“å°å“åº”è´¨é‡ç»Ÿè®¡ä¿¡æ¯")
    except Exception as e:
        social_log.error(f"æ‰“å°å“åº”è´¨é‡ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")

    # å…³é—­è®¤çŸ¥å¼•å¯¼å¼•æ“
    if cge_controller:
        try:
            cge_controller.shutdown()
            social_log.info("è®¤çŸ¥å¼•å¯¼å¼•æ“å·²å…³é—­")
        except Exception as e:
            social_log.error(f"å…³é—­è®¤çŸ¥å¼•å¯¼å¼•æ“å¤±è´¥: {e}")

    social_log.info("Simulation finish!")


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # ç¦ç”¨æ‰€æœ‰ä¸å¼‚æ­¥ç›¸å…³çš„è¿è¡Œæ—¶è­¦å‘Š
    warnings.simplefilter("ignore", RuntimeWarning)

    # è®¾ç½®å¼‚æ­¥æ—¥å¿—ä¸ºERRORçº§åˆ«ï¼ŒæŠ‘åˆ¶WARNINGä¿¡æ¯
    asyncio.get_event_loop().set_debug(False)
    logging.getLogger("asyncio").setLevel(logging.ERROR)

    args = parser.parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    log_dir, platform_log, agent_log, social_log = setup_logging()
    
    # åŠ è½½é…ç½®æ–‡ä»¶å¹¶ä¿å­˜åˆ°æ—¥å¿—ç›®å½•
    cfg = load_and_save_config(args.config_path)
    
    # è®¾ç½®éšæœºç§å­
    random_seed = setup_random_seed(args, args.config_path)


    # è·å–å„éƒ¨åˆ†é…ç½®
    data_params = cfg.get("data", {})  # è·å–æ•°æ®å‚æ•°
    simulation_params = cfg.get("simulation", {})  # è·å–æ¨¡æ‹Ÿå‚æ•°
    model_configs = cfg.get("model", {})  # è·å–æ¨¡å‹é…ç½®
    inference_params = cfg.get("inference", {})  # è·å–æ¨ç†å‚æ•°
    guidance_engine_config = cfg.get("guidance_engine", {})  # è·å–å¼•å¯¼å¼•æ“é…ç½®
    guidance_tasks_config = cfg.get("guidance_tasks", [])  # è·å–å¼•å¯¼ä»»åŠ¡é…ç½®
    
    # æ‰“å°é…ç½®ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    print(f"{Fore.CYAN}===== ASCEç³»ç»Ÿé…ç½®ä¿¡æ¯ ====={Fore.RESET}")
    print(f"æ•°æ®é›†: {data_params.get('data_name', 'N/A')}")
    print(f"æ¨¡å‹ç±»å‹: {inference_params.get('model_type', 'N/A')}")
    print(f"æ™ºèƒ½ä½“æ•°é‡: {simulation_params.get('num_agents', 'N/A')}")
    print(f"æ—¶é—´æ­¥æ•°: {simulation_params.get('num_timesteps', 'N/A')}")
    print(f"æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"{Fore.CYAN}==============================={Fore.RESET}")
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä¸ªä½“æ¨¡æ‹Ÿæ¨¡å¼
    individual_mode = simulation_params.get("individual_mode", False)
    label_mode = simulation_params.get("label_mode", False)
    print(f"individual_mode: {individual_mode}, label_mode: {label_mode}")
    print(f"{Fore.GREEN}Using Standard Mode: {Fore.RESET}")
    
    # è¿è¡Œæ ‡å‡†æ¨¡æ‹Ÿ
    asyncio.run(
        normal_running(
            data_params=data_params,
            model_configs=model_configs,
            inference_configs=inference_params,
            simulation_params=simulation_params,
            guidance_engine_config=guidance_engine_config,  # ä¼ é€’å¼•å¯¼å¼•æ“é…ç½®
            guidance_tasks_config=guidance_tasks_config,    # ä¼ é€’å¼•å¯¼ä»»åŠ¡é…ç½®
            random_seed=random_seed,  # ä¼ é€’éšæœºç§å­
        ),
        debug=True,  # å¯ç”¨è°ƒè¯•æ¨¡å¼
    )
    print(f"Simulation Finish !!")
    # è¾“å‡ºæ—¥å¿—è·¯å¾„ã€ä¿å­˜çš„æ–‡ä»¶è·¯å¾„


